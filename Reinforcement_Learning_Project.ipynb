{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c11545-4458-4dde-86a6-01257cacae62",
   "metadata": {},
   "source": [
    "Reinforcement Learning Assignment\n",
    "\n",
    "Group: 07\n",
    "\n",
    "Authors: Paul Bonnie, Nicolas Forcella, Zheyan Lin, Tijmen Westeneng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6954c76-bfc3-4af1-804f-53dfebee5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from typing import List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b0c02-8719-48ec-a06e-b9677aa7243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "class ZombieEscapeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"ansi\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=8, fixed_seed: Optional[int] = None, gamma=1):\n",
    "        super(ZombieEscapeEnv, self).__init__()\n",
    "        #store grid size\n",
    "        self.grid_size = size\n",
    "        self.fixed_seed = fixed_seed\n",
    "        #PYGAME INITIALIZATIONS\n",
    "        #only load this if needed\n",
    "        if render_mode == 'human':\n",
    "            self.window_size = 512  # The size of the PyGame window\n",
    "            self.brain_img = pygame.image.load(\"./img/brain.png\")  # Brain\n",
    "            self.grass_img = pygame.image.load(\"img/grass.jpeg\")  # grass\n",
    "            self.spikeweed_img = pygame.image.load(\"./img/spikeweed.png\")  # spikeweed\n",
    "            self.plant_img = pygame.image.load(\"./img/plant.png\")  # plant\n",
    "            self.zombie_img = pygame.image.load(\"./img/zombie.png\")  # zombie\n",
    "            self.house_img = pygame.image.load(\"./img/house.png\")  # house\n",
    "\n",
    "            #Resize images to fit grid size\n",
    "            self.cell_size = self.window_size // self.grid_size\n",
    "            self.brain_img = pygame.transform.scale(self.brain_img, (self.cell_size, self.cell_size))\n",
    "            self.grass_img = pygame.transform.scale(self.grass_img, (self.cell_size, self.cell_size))\n",
    "            self.spikeweed_img = pygame.transform.scale(self.spikeweed_img, (self.cell_size, self.cell_size))\n",
    "            self.plant_img = pygame.transform.scale(self.plant_img, (self.cell_size, self.cell_size))\n",
    "            self.zombie_img = pygame.transform.scale(self.zombie_img, (self.cell_size, self.cell_size))\n",
    "            self.house_img = pygame.transform.scale(self.house_img, (self.cell_size, self.cell_size))\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        #ACTUAL GAME INITIALIZATIONS\n",
    "        self.r_map = self.generate_random_map(seed=self.fixed_seed)\n",
    "        self.nrow, self.ncol = nrow, ncol = self.r_map.shape\n",
    "        self.gamma = gamma\n",
    "        nA = 4  # actions\n",
    "        nS = nrow * ncol  # states\n",
    "\n",
    "        # Create tuples inside the dictionary for each cell, to store information like this:\n",
    "        # (transition probability, next state, reward, terminated)\n",
    "        self.P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        self.s = 0\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        # Change the position of the agent to an index\n",
    "        def to_s(row, col):\n",
    "            return row * ncol + col\n",
    "\n",
    "        # Constrain the movement of zombie so it doesn't go out of index\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        # define the reward and new states after action so it can be filled in self.P\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            new_row, new_col = inc(row, col, action)\n",
    "            new_state = to_s(new_row, new_col)\n",
    "            new_letter = self.r_map[new_row, new_col]\n",
    "            terminated = new_letter in \"CD\"\n",
    "            reward = self.get_reward(new_letter)\n",
    "            \n",
    "            return new_state, reward, terminated\n",
    "\n",
    "        # Fill in self.P\n",
    "        probability_table = [0.1, 0.8, 0.1]\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = self.P[s][a]\n",
    "                    letter = self.r_map[row, col]\n",
    "                    # if current state is \"C\" or \"D\", game over\n",
    "                    if letter in \"CD\":\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        # for other state, for each action, the probability of going in the right direction\n",
    "                        # is 0.8, going in the correct direction's left/right's probability is 0.1\n",
    "                        for i, b in enumerate([(a - 1) % 4, a, (a + 1) % 4]):\n",
    "                            li.append((probability_table[i], *update_probability_matrix(row, col, b)))\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "    def get_gamma(self):\n",
    "        return self.gamma\n",
    "    \n",
    "    def is_valid(self, board: List[List[str]], max_size: int) -> bool:\n",
    "        # use simple dfs to track there's a valid path from start to Dave's house\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    # exceeding the index\n",
    "                    if r_new < 0 or r_new >= max_size or c_new < 0 or c_new >= max_size:\n",
    "                        continue\n",
    "                    # Dave's house\n",
    "                    if board[r_new][c_new] == \"D\":\n",
    "                        return True\n",
    "                    # To avoid zombie being devoured by the chompers\n",
    "                    if board[r_new][c_new] != \"C\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    def generate_random_map(\n",
    "            self, size: int = 8, seed: Optional[int] = None\n",
    "    ) -> np.array:\n",
    "        \"\"\"Generates a random valid map (one that has a path from start to Dave's house)\n",
    "\n",
    "        Args:\n",
    "            size: size of each side of the grid\n",
    "            seed: optional seed to ensure the generation of reproducible maps\n",
    "\n",
    "        Returns:\n",
    "            A random valid map\n",
    "        \"\"\"\n",
    "        valid = False\n",
    "        board = []  # initialize to make pyright happy\n",
    "        p1 = 0.7\n",
    "        p2 = 0.1\n",
    "        p3 = 0.1\n",
    "        p4 = 0.1\n",
    "\n",
    "        np_random, _ = seeding.np_random(seed if seed is not None else self.fixed_seed)\n",
    "\n",
    "        while not valid:\n",
    "            board = np_random.choice([\"L\", \"W\", \"C\", \"B\"], (size, size), p=[p1, p2, p3, p4])\n",
    "            board[0][0] = \"S\"\n",
    "            board[-1][-1] = \"D\"\n",
    "            valid = self.is_valid(board, size)\n",
    "        return board\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)  # choose the next move based on the probability table of this action\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return int(s), r, t, False, {\"prob\": p}\n",
    "    \n",
    "    \n",
    "    def _get_agent_location(self):\n",
    "        #convert state to row, column\n",
    "        return self.convert_state_to_table_index(self.s)\n",
    "    \n",
    "    def convert_state_to_table_index(self, state):\n",
    "        return state // self.grid_size, state % self.grid_size\n",
    "    \n",
    "    def get_reward(self, letter):\n",
    "        # decide reward\n",
    "        reward:float\n",
    "        if letter == \"B\":\n",
    "            reward = 0.5\n",
    "        elif letter == \"W\":\n",
    "            reward = -0.5\n",
    "        elif letter == \"D\":\n",
    "            reward = 100\n",
    "        elif letter == \"C\":\n",
    "            reward = -1000  \n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_letter(self, state):\n",
    "        row, column =  self.convert_state_to_table_index(state)\n",
    "        return self.r_map[row, column]\n",
    "    \n",
    "    def get_state_reward(self, state):\n",
    "        letter = self.get_letter(state)\n",
    "        return self.get_reward(letter)\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return self.s\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None, ):\n",
    "        #super().reset(seed = seed if seed is not None else self.fixed_seed)\n",
    "        self.s = 0\n",
    "        return int(self.s)\n",
    "\n",
    "        # Initialize the state\n",
    "        # self.state = self.get_state()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.grid_size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the house\n",
    "        target_x, target_y = (self.grid_size - 1, self.grid_size - 1)\n",
    "        canvas.blit(self.house_img, (target_x * pix_square_size, target_y * pix_square_size))\n",
    "\n",
    "        #Draw the rest of the objects\n",
    "        for row in range(self.r_map.shape[0]):\n",
    "            for col in range(self.r_map.shape[1]):\n",
    "                letter = self.r_map[col, row]\n",
    "                match letter:\n",
    "                    case 'B':\n",
    "                        canvas.blit(self.grass_img, (row * pix_square_size, col * pix_square_size))\n",
    "                        canvas.blit(self.brain_img, (row * pix_square_size, col * pix_square_size))\n",
    "                    case 'L':\n",
    "                        canvas.blit(self.grass_img, (row * pix_square_size, col * pix_square_size))\n",
    "                    case 'C':\n",
    "                        canvas.blit(self.grass_img, (row * pix_square_size, col * pix_square_size))\n",
    "                        canvas.blit(self.plant_img, (row * pix_square_size, col * pix_square_size))\n",
    "                    case 'W':\n",
    "                        canvas.blit(self.grass_img, (row * pix_square_size, col * pix_square_size))\n",
    "                        canvas.blit(self.spikeweed_img, (row * pix_square_size, col * pix_square_size))\n",
    "       \n",
    "        # Now we draw the agent\n",
    "        agent_y, agent_x  = self._get_agent_location()\n",
    "        canvas.blit(self.zombie_img, (agent_x * pix_square_size, agent_y * pix_square_size))\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.grid_size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to\n",
    "            # keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def render(self):\n",
    "        # Print the board by placing the agent at the specified position\"\"\"\n",
    "        if self.render_mode == 'human':\n",
    "            return self._render_frame()\n",
    "        \n",
    "        if self.render_mode == 'ansi':\n",
    "            agent_row, agent_col = int(self.s) // self.grid_size, int(self.s) % self.grid_size\n",
    "            r_map_copy = self.r_map.copy()\n",
    "            r_map_copy[agent_row, agent_col] = 'Z'  # Place agent at the specified position\n",
    "            for row in r_map_copy:\n",
    "                print(\"\".join(row for row in row))\n",
    "            print(\"\")\n",
    "\n",
    "    def is_terminal(self, state=None):\n",
    "        if state == None:\n",
    "          state = self.s\n",
    "\n",
    "        # Check current state to see if it is terminal\n",
    "        agent_row, agent_col = int(state) // self.grid_size, int(state) % self.grid_size\n",
    "        r_map_copy = self.r_map.copy()\n",
    "        letter = r_map_copy[agent_row, agent_col]\n",
    "        if letter in 'CD':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def generate_episode(self, policy, initial_state=0, initial_action = None, max_steps = 50):\n",
    "        \"\"\"\n",
    "        Sample an episode from the env following a policy. (Start at state 0 unless the intial state is specified)\n",
    "        \"\"\"\n",
    "\n",
    "        state = initial_state\n",
    "        self.s = initial_state\n",
    "        episode = []\n",
    "        if initial_action == None:\n",
    "            action = policy[state]\n",
    "        else:\n",
    "            action = initial_action\n",
    "        terminal = False\n",
    "        steps = 0\n",
    "        truncated = False\n",
    "\n",
    "        while not terminal and steps < max_steps: # Generate episodes where the actions are performed following the policy until in a terminal state\n",
    "            \n",
    "            next_state, reward, terminal, _, _ = self.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            state = next_state\n",
    "            action = policy[state]\n",
    "            steps += 1\n",
    "        \n",
    "        if steps >= max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        return episode, truncated\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.window = None\n",
    "            self.clock = None  # Reset clock too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461ba19-f741-4289-bda3-9e410a230f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAlgorithm:\n",
    "    \"\"\"\n",
    "    This class represents the base case for a learning algorithm. It contains methods shared by all algorithms such as getters and plotting.\n",
    "    \"\"\"\n",
    "    def __init__(self, zombie_environment:ZombieEscapeEnv, episodes = None, target_values = None):\n",
    "        #INITIALIZE CLASS VAR\n",
    "        self.trained = False\n",
    "        self.value_function = None\n",
    "        self.policy = None\n",
    "        #INITIALIZE ENVIRONMENT VALUES\n",
    "        self.zombie_environment = zombie_environment\n",
    "        self.number_of_actions = zombie_environment.action_space.n\n",
    "        self.number_of_states = zombie_environment.observation_space.n\n",
    "        self.gamma = self.zombie_environment.get_gamma()\n",
    "\n",
    "        # Target values and error array for RMSE calculations and plotting\n",
    "        if episodes is not None and target_values is not None:\n",
    "            self.episodes = episodes\n",
    "            self.target_values = target_values\n",
    "            self.errors = np.zeros(episodes)\n",
    "\n",
    "        # Initialize list consisting of tuples of episode number and cumulative reward for that episode\n",
    "        self.cum_reward_list = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__\n",
    "\n",
    "    def run_training(self):\n",
    "        pass\n",
    "\n",
    "    def get_training_results(self):\n",
    "        #If model has not been trained\n",
    "        if not self.trained:\n",
    "            self.run_training()\n",
    "            self.trained = True\n",
    "        \n",
    "        return self.value_function, self.policy\n",
    "    \n",
    "    def visualise_policy(self):\n",
    "        \"\"\"\n",
    "        Plots the policy on an 8x8 grid. For each state an arrow is plotted in the direction \n",
    "        specified by the policy. For terminal states, the name of the terminal state, e.g. Chomper,\n",
    "        is plotted instead of an arrow.\n",
    "        \"\"\"\n",
    "        policy_matrix = self.policy.reshape(8,8)\n",
    "\n",
    "        arrows = {0:(-1,0), 1:(0,-1), 2:(1,0), 3:(0,1)} # For each action, we define the x and y direction of the arrow (e.g., for action 0, i.e. left, the arrow should point in direction (-1, 0))\n",
    "        scale = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        ax.set_xlim(-0.5, 7.5)\n",
    "        ax.set_ylim(-0.5, 7.5)\n",
    "        ax.set_xticks(np.arange(8) - 0.5)\n",
    "        ax.set_yticks(np.arange(8) - 0.5)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                state_id = r * 8 + c\n",
    "                # Check if the state is terminal, if yes plot an arrow, if no plot a T\n",
    "                if not self.zombie_environment.is_terminal(state_id):\n",
    "                    action = policy_matrix[r, c]\n",
    "                    dx, dy = arrows[action] # Get the arrow x and y directions\n",
    "                    ax.arrow(c, 7 - r, dx * scale, dy * scale, head_width=0.2, head_length=0.2, fc='blue', ec='blue') # create an arrow for the cell \n",
    "                elif self.zombie_environment.get_letter(state_id) == \"C\":\n",
    "                    ax.text(c, 7 - r, \"Chomper\", fontsize=10, ha='center', va='center')\n",
    "                elif self.zombie_environment.get_letter(state_id) == \"D\":\n",
    "                    ax.text(c, 7 - r, \"Dave's\\nhouse\", fontsize=10, ha='center', va='center')\n",
    "\n",
    "        ax.set_title('Policy')\n",
    "        ax.set_aspect('equal') # ensure that scaling of x and y is equal so the grid remains square\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_values(self, title='Value Function', value_function=None):\n",
    "        \"\"\"\n",
    "        Plots the value matrix on an 8x8 grid.\n",
    "        \"\"\"\n",
    "        if value_function is None:\n",
    "            value_function = self.value_function\n",
    "\n",
    "        value_matrix = np.flip(value_function.reshape(8,8), axis=0) # Flip the rows so row 0 is at the bottom and row 7 is at the top to be in line with the pygame environment\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        ax.set_xlim(-0.5, 7.5)\n",
    "        ax.set_ylim(-0.5, 7.5)\n",
    "        ax.set_xticks(np.arange(8) - 0.5)\n",
    "        ax.set_yticks(np.arange(8) - 0.5)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                value = value_matrix[r, c].round(2)\n",
    "                ax.text(c, r, value, ha='center', va='center')\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_aspect('equal') # ensure that scaling of x and y is equal so the grid remains square\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_values_heatmap(self, title='Value Function', value_function=None):\n",
    "        \"\"\"\n",
    "        Plot the value matrix on an 8x8 grid as a heatmap.\n",
    "        \"\"\"\n",
    "        if value_function is None:\n",
    "            value_function = self.value_function\n",
    "\n",
    "        value_matrix = np.flip(value_function.reshape(8,8), axis=0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        im = ax.imshow(value_matrix, origin='lower')\n",
    "\n",
    "        ax.set_xlim(-0.5, 7.5)\n",
    "        ax.set_ylim(-0.5, 7.5)\n",
    "        ax.set_xticks(np.arange(8) - 0.5)\n",
    "        ax.set_yticks(np.arange(8) - 0.5)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linestyle='--', color='k', alpha=0.7)\n",
    "\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                value = value_matrix[r, c].round(2)\n",
    "                ax.text(c, r, value, ha='center', va='center', color='w')\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_values_difference(self, values_comparison = None, abs = False, heatmap = False):\n",
    "        \"\"\"\n",
    "        Visualise difference in value function matrices on an 8x8 grid.\n",
    "        Args:\n",
    "            values_comparison (np.array): The value function to compare the own value function to\n",
    "            abs (bool): If the difference should be represented as absolute values (empty: False)\n",
    "            heatmap (bool): If the difference should be plotted as a heatmap (empty: False)\n",
    "        \"\"\"\n",
    "        if values_comparison is None:\n",
    "            assert self.target_values is not None, \"visualise_values_difference: target_values must be set when no values_comparison is given\"\n",
    "            values_comparison = self.target_values\n",
    "\n",
    "        subtraction_matrix = np.subtract(self.value_function, values_comparison)\n",
    "\n",
    "        if abs:\n",
    "            subtraction_matrix = np.abs(subtraction_matrix)\n",
    "\n",
    "        if heatmap:\n",
    "            self.visualise_values_heatmap(title=\"Value Function Difference\", value_function=subtraction_matrix)\n",
    "        else:\n",
    "            self.visualise_values(title='Value Function Difference', value_function=subtraction_matrix)\n",
    "\n",
    "    def calc_policy_reward(self, episode_n):\n",
    "        \"\"\"\n",
    "        Calculates and saves cumulative reward for current policy.\n",
    "        TODO: Allow for non-deterministic policies\n",
    "        \"\"\"\n",
    "        cum_reward = 0\n",
    "\n",
    "        state = self.zombie_environment.reset()\n",
    "        state_n = 0\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            action = self.policy[state]\n",
    "            next_state, reward, terminal = self.zombie_environment.step(action)[:3]\n",
    "            # TODO: Check if this cumulative reward calc is correct\n",
    "            cum_reward += self.gamma ** state_n * reward\n",
    "            state = next_state\n",
    "\n",
    "            # To prevent infinite loops, episodes max out at 1000 visited states\n",
    "            state_n += 1\n",
    "            if state_n > 1000:\n",
    "                cum_reward = -100\n",
    "                break\n",
    "\n",
    "        self.cum_reward_list.append((episode_n, cum_reward))\n",
    "\n",
    "    def store_error(self, episode_number):\n",
    "        \"\"\"Calculate Mean Squared Error and save to error array\"\"\"\n",
    "        self.errors[episode_number] = np.sqrt(np.mean((self.value_function - self.target_values) ** 2))\n",
    "\n",
    "    def plot_error(self):\n",
    "        \"\"\"Plot Mean Squared Error over episodes\"\"\"\n",
    "        x = list(range(len(self.errors)))\n",
    "\n",
    "        plt.plot(x, self.errors)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_cum_reward(self):\n",
    "        if len(self.cum_reward_list) > 0:\n",
    "            index, cum_reward_list = zip(*self.cum_reward_list)\n",
    "            plt.scatter(index, cum_reward_list)\n",
    "            plt.xlabel(\"Episode number\")\n",
    "            plt.ylabel(\"Cumulative reward\")\n",
    "            plt.title(\"Cumulative reward of policy over episodes\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c465c-4532-4a40-b33b-93d7e37584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(LearningAlgorithm):\n",
    "    def __init__(self, zombie_environment, theta):\n",
    "        super().__init__(zombie_environment=zombie_environment)\n",
    "        #INITIALIZE CONFIG PARAMETERS\n",
    "        self.theta = theta\n",
    "        #INITIALIZE POLICY PROPERTY\n",
    "        action = self.zombie_environment.action_space.sample()\n",
    "        self.policy = np.zeros(self.number_of_states, dtype= 'int')\n",
    "        self.policy += action\n",
    "        #INITIALIZE VALUE FUNCTION\n",
    "        self.value_function = np.zeros(self.number_of_states)\n",
    "        \n",
    "\n",
    "                \n",
    "    def policy_evaluation(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            #initialize new array to store evaluated values\n",
    "            new_value_function = np.zeros(self.number_of_states)\n",
    "            #iterate for all postion in the grid and update value\n",
    "            for state in range(self.number_of_states):\n",
    "                original_action = self.policy[state]\n",
    "                #Avoid updating the values of terminal states\n",
    "                if self.zombie_environment.is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                #initialize value_update to calculate the sum of V(pi) for 4 directions\n",
    "                value_update = 0\n",
    "                #implementing Bellman expectation equation\n",
    "                for prob in self.zombie_environment.P[state][original_action]:\n",
    "                    #value_update += prob[0]*(self.zombie_environment.get_state_reward(state) + self.gamma*self.value_function[prob[1]])\n",
    "                    value_update += prob[0]*(prob[2] + self.gamma*self.value_function[prob[1]])\n",
    "                \n",
    "                #store the updated value\n",
    "                new_value_function[state] = value_update\n",
    "                delta = max(delta, abs(self.value_function[state] - new_value_function[state]))\n",
    "            \n",
    "            self.value_function = new_value_function\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return \n",
    "    \n",
    "    def policy_improve(self):\n",
    "        while True:\n",
    "            #first perform policy evaluation\n",
    "            self.policy_evaluation()\n",
    "            \n",
    "            policy_stable = True\n",
    "            \n",
    "            for state in range(self.number_of_states):\n",
    "                #avoid updating terminal states\n",
    "                if self.zombie_environment.is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                original_action = self.policy[state]\n",
    "                q_value = np.zeros(self.number_of_actions)\n",
    "                \n",
    "                #find the best direction(highest expected reward) in current position\n",
    "                for action in range(self.number_of_actions):\n",
    "                    #initialize q to calculate q value for each action\n",
    "                    q_value_action = 0\n",
    "                    for prob in self.zombie_environment.P[state][action]:\n",
    "                        #q_value_action += prob[0]*(self.zombie_environment.get_state_reward(state) + self.gamma*self.value_function[prob[1]])\n",
    "                        q_value_action += prob[0]*(prob[2] + self.gamma*self.value_function[prob[1]])\n",
    "                    #update q_value array, the index of the array corresponds to the current direction\n",
    "                    q_value[action] = q_value_action\n",
    "                \n",
    "                self.policy[state] = np.argmax(q_value)\n",
    "                #check if the policy array already store the optimal position\n",
    "                #if not cotinue the loop\n",
    "                if original_action != self.policy[state]:\n",
    "                    policy_stable = False\n",
    "                    \n",
    "            if policy_stable == True:\n",
    "                break\n",
    "        return self.value_function, self.policy\n",
    "    \n",
    "    def run_training(self):\n",
    "        self.policy_improve()\n",
    "        self.trained = True\n",
    "        return self.value_function, self.policy\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11500903-40ae-43d8-b8c3-836450900e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(LearningAlgorithm):\n",
    "    \"\"\"\n",
    "    Inherits learning algorithms and impleemnts value iteration algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, zombie_environment, theta):\n",
    "        super().__init__(zombie_environment=zombie_environment)\n",
    "        #INITIALIZE CONFIG PARAMETERS\n",
    "        self.theta = theta\n",
    "        #INITIALIZE POLICY PROPERTY\n",
    "        self.policy = np.zeros(self.number_of_states, dtype= 'int')\n",
    "        #INITIALIZE VALUE FUNCTION\n",
    "        self.value_function = np.zeros(self.number_of_states)\n",
    "        \n",
    "    \n",
    "    def single_value_iteration(self):\n",
    "        \"\"\"\n",
    "        Implement a single value function iteration.\n",
    "        Returns: delta: the value of delta after the iteration\n",
    "        \"\"\"\n",
    "        #Initialize q values function to store the values for computing max\n",
    "        new_value_function = np.zeros(self.number_of_states)\n",
    "        delta = 0\n",
    "        for state in range(self.number_of_states):                \n",
    "            #Avoid updating the values of terminal states\n",
    "            if self.zombie_environment.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            values = np.zeros(self.number_of_actions) #Store the values of the different actions\n",
    "            for action in range(self.number_of_actions):\n",
    "                value = 0\n",
    "                # prob is a tuple of (transition probability, next state, reward)\n",
    "                for prob in self.zombie_environment.P[state][action]:\n",
    "                    value += prob[0]*(prob[2] + self.gamma*self.value_function[prob[1]]) #The value for an action\n",
    "                \n",
    "                values[action] = value\n",
    "            #Get max value, best action and store the value and best action\n",
    "            max_value = np.max(values, axis=0)\n",
    "            best_action = np.argmax(values)\n",
    "            self.policy[state] = best_action\n",
    "            new_value_function[state] = max_value\n",
    "            #update delta\n",
    "            delta = max(delta, abs(self.value_function[state] - new_value_function[state]))\n",
    "\n",
    "        #update policy\n",
    "        self.value_function = new_value_function\n",
    "        \n",
    "        return delta\n",
    "                \n",
    "                \n",
    "    def run_training(self):\n",
    "        \"\"\"\n",
    "        Implement the method for running a training. Train the agent until delta is smaller or equal than theta.\n",
    "        \"\"\"\n",
    "        #Run value iteration until convergence\n",
    "        delta = self.single_value_iteration()\n",
    "        while delta > self.theta:\n",
    "            delta = self.single_value_iteration()\n",
    "\n",
    "        #Given states find optimal policy\n",
    "        self.trained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f991a38-f901-4139-bf8e-53b0eec508da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloLearning(LearningAlgorithm):\n",
    "        \n",
    "    def __init__(self, zombie_environment, max_steps=50, target_values=None, episodes=100):\n",
    "        super().__init__(zombie_environment=zombie_environment)\n",
    "\n",
    "        self.state_action_value_function = None #The action value function np array [state, action]\n",
    "        self.state_action_returns = None #Sum of rewards collected for the state action pair, np array [state, action]\n",
    "        self.count_state_action_visits = None #Count of visits for the state action pair, np array [state, action]\n",
    "        self.max_steps = max_steps\n",
    "        self.state_returns = None #Sum of rewards collected for the state action pair, np array [state, action]\n",
    "        self.count_state_visits = None #Count of visits for the state action pair, np array [state, action]\n",
    "        self.target_values = target_values\n",
    "        self.episodes = episodes\n",
    "        self.errors = np.zeros(self.episodes)\n",
    "\n",
    "    def calculate_expected_return(self, episode, mode, gamma):\n",
    "        \"\"\"\n",
    "        Calculate first-visit G_t recursively and update the count of visited + returns \n",
    "        \n",
    "        Parameters:\n",
    "        episode: a list of tuples containing the rewards.\n",
    "        mode: state or state-action\n",
    "        \"\"\"\n",
    "        #Improve efficeicncy by first doing forward pass and then check if value needs to be added\n",
    "        episode_length = len(episode)\n",
    "        returns = np.zeros(episode_length)\n",
    "        \n",
    "        G = 0\n",
    "\n",
    "        #Backwards\n",
    "        for t in range(episode_length - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            returns[t] = G\n",
    "\n",
    "        #Forward pass\n",
    "        visited_states = set()\n",
    "        visited_state_actions = set()\n",
    "\n",
    "        for t, episode_step in enumerate(episode):\n",
    "            \n",
    "            state, action, reward = episode_step\n",
    "            \n",
    "            if mode == \"state_value\":\n",
    "                if state not in visited_states:\n",
    "                    visited_states.add(state)\n",
    "                    self.state_returns[state] += returns[t]\n",
    "                    self.count_state_visits[state] += 1\n",
    "\n",
    "            elif mode == \"state_action_value\":\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.state_action_returns[state, action] += returns[t]\n",
    "                    self.count_state_action_visits[state, action] += 1\n",
    "        \n",
    "    def store_error(self, episode_number):\n",
    "        #calculate squared error\n",
    "        self.errors[episode_number] = np.sqrt(np.mean( (self.value_function - self.target_values) ** 2 ))\n",
    "\n",
    "    def plot_error(self):\n",
    "        x = list(range(len(self.errors)))\n",
    "        plt.plot(x, self.errors)\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"episodes\")\n",
    "        plt.ylabel(\"mean squared error\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f5b1b-1700-4b89-8938-cd4946e17736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloPrediction(MonteCarloLearning):\n",
    "\n",
    "    def __init__(self, zombie_environment, policy, episodes = 100, max_steps = 50, target_values=None):\n",
    "        super().__init__(zombie_environment=zombie_environment, max_steps=max_steps, episodes=episodes, target_values=target_values)\n",
    "        self.episodes = episodes\n",
    "        self.policy = policy\n",
    "        self.state_action_value_function = np.zeros((self.number_of_states, self.number_of_actions)) #The action value function np array [state, action]\n",
    "        self.state_action_returns = np.zeros((self.number_of_states, self.number_of_actions)) #Sum of rewards collected for the state action pair, np array [state, action]\n",
    "        self.count_state_action_visits = np.zeros((self.number_of_states, self.number_of_actions), dtype='int') #Count of visits for the state action pair, np array [state, action]\n",
    "\n",
    "    \n",
    "    def run_training(self):\n",
    "        \"\"\"\n",
    "        Process:\n",
    "\n",
    "        1 - For each episode do.\n",
    "            1- random initial state selection \n",
    "            2 - episode generation based on policy and initial state\n",
    "            3 - update the count and sum rewards (calling g unction with mode)\n",
    "            \n",
    "        2 - compute average\n",
    "        \n",
    "        \"\"\"\n",
    "        for episode_number in range(self.episodes):\n",
    "            #Only generate complete episodes for monte carlo methods\n",
    "            truncated = True\n",
    "            while truncated:\n",
    "                # Generate a random non-terminal starting state and random action\n",
    "                random_state = np.random.randint(0, self.number_of_states)\n",
    "                while self.zombie_environment.is_terminal(random_state):\n",
    "                    random_state = np.random.randint(0, self.number_of_states)\n",
    "\n",
    "                random_action = np.random.randint(0, self.number_of_actions)\n",
    "\n",
    "                # generate an episode starting at the random starting state and following the policy\n",
    "                episode, truncated = self.zombie_environment.generate_episode(policy=self.policy, initial_state=random_state, initial_action=random_action, max_steps=self.max_steps)\n",
    "                \n",
    "            # update count and reward using calculate_expected_return\n",
    "            self.calculate_expected_return(episode=episode, gamma=self.gamma, mode=\"state_action_value\")\n",
    "           \n",
    "            self.state_action_value_function = np.divide(\n",
    "                self.state_action_returns, \n",
    "                self.count_state_action_visits,\n",
    "                out=self.state_action_value_function,\n",
    "                where=self.count_state_action_visits>0\n",
    "            )\n",
    "\n",
    "            self.value_function = np.max(self.state_action_value_function, axis=1)\n",
    "\n",
    "            \n",
    "            if self.target_values is not None:\n",
    "                self.store_error(episode_number)\n",
    "\n",
    "            print(f\"Episode: {episode_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94d40c-30c7-4dce-9645-4437d06de1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloControl(MonteCarloLearning):\n",
    "\n",
    "    def __init__(self, zombie_environment, episodes = 100, max_steps = 50, target_values=None):\n",
    "        super().__init__(zombie_environment = zombie_environment, max_steps=max_steps, target_values=target_values, episodes=episodes)\n",
    "        self.episodes = episodes\n",
    "        self.policy = np.zeros(self.number_of_states, dtype='int')\n",
    "\n",
    "        self.state_action_value_function = np.zeros((self.number_of_states, self.number_of_actions)) #The action value function np array [state, action]\n",
    "        self.state_action_returns = np.zeros((self.number_of_states, self.number_of_actions)) #Sum of rewards collected for the state action pair, np array [state, action]\n",
    "        self.count_state_action_visits = np.zeros((self.number_of_states, self.number_of_actions), dtype='int') #Count of visits for the state action pair, np array [state, action]\n",
    "\n",
    "    \n",
    "    def run_training(self):\n",
    "        \"\"\"\n",
    "        Init:\n",
    "        random policy for non terminal states\n",
    "        q(s, a) init in 0\n",
    "        initialize returns matrix\n",
    "\n",
    "        Process:\n",
    "\n",
    "        1 - Choose random start from non terminal states.\n",
    "        2 - Choose random initial action\n",
    "        3 - Generate random episode following policy and with inital states\n",
    "        4 - run the function for calculating returns\n",
    "        5 - update policy.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        for episode_number in tqdm(range(self.episodes)):\n",
    "            #Only generate complete episodes for monte carlo methods, avoid them being too long\n",
    "            truncated = True\n",
    "            while truncated:\n",
    "                #Generate random state that is not terminal\n",
    "                random_state = np.random.randint(0, self.number_of_states)\n",
    "                while self.zombie_environment.is_terminal(random_state):\n",
    "                    random_state = np.random.randint(0, self.number_of_states)\n",
    "                \n",
    "                random_action = np.random.randint(0, self.number_of_actions)\n",
    "                # generate episode with random start\n",
    "                episode, truncated = self.zombie_environment.generate_episode(policy=self.policy, initial_state=random_state, initial_action=random_action, max_steps=self.max_steps)\n",
    "                # update count and reward using calculate_expected_return\n",
    "            \n",
    "            self.calculate_expected_return(episode=episode, gamma=self.gamma, mode=\"state_action_value\")\n",
    "            \n",
    "            self.state_action_value_function = np.divide(\n",
    "                self.state_action_returns, \n",
    "                self.count_state_action_visits,\n",
    "                out=self.state_action_value_function,\n",
    "                where=self.count_state_action_visits>0\n",
    "            )\n",
    "\n",
    "            self.value_function = np.max(self.state_action_value_function, axis=1)\n",
    "            self.policy = np.argmax(self.state_action_value_function, axis=1)\n",
    "\n",
    "            # Every x episodes calculate the cumulative reward of the current policy\n",
    "            if episode_number % round(self.episodes / 1000) == 0:\n",
    "                self.calc_policy_reward(episode_n=episode_number)\n",
    "            \n",
    "            if self.target_values is not None:\n",
    "                self.store_error(episode_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d800c5-cb92-445f-b760-bb57960b8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_Prediction(LearningAlgorithm):\n",
    "    def __init__(self, zombie_environment:ZombieEscapeEnv, alpha, policy, episodes = 1000, target_values=None):\n",
    "        super().__init__(zombie_environment=zombie_environment)\n",
    "        self.episodes = episodes\n",
    "        self.alpha = alpha\n",
    "        self.policy = policy\n",
    "        self.target_values = target_values\n",
    "        self.errors = np.zeros(self.episodes)\n",
    "        #initial state, action values\n",
    "        self.Q_S_A = np.zeros((self.number_of_states, self.number_of_actions))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def run_training(self):\n",
    "        for episode in range(self.episodes):\n",
    "            state, info = self.zombie_environment.reset()\n",
    "            terminated =  False\n",
    "            action = self.policy[state]\n",
    "            \n",
    "            while not terminated:\n",
    "                next_state, reward, terminated = self.zombie_environment.step(action)[:3]\n",
    "                next_action = self.policy[next_state]\n",
    "                self.Q_S_A[state][action] += self.alpha * (reward + self.gamma * self.Q_S_A[next_state][next_action] -  self.Q_S_A[state][action])\n",
    "                state, action = next_state, next_action\n",
    "\n",
    "            self.value_function = np.max(self.Q_S_A, axis=1)\n",
    "            if self.target_values is not None:\n",
    "                self.store_error(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df4d15-8b4a-4ee7-9080-482ffecf0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(LearningAlgorithm):\n",
    "    def __init__(self, zombie_environment:ZombieEscapeEnv, episodes = 1000, alpha = 0.1, target_values=None):\n",
    "        super().__init__(zombie_environment=zombie_environment)\n",
    "        self.episodes = episodes\n",
    "        self.alpha = alpha\n",
    "        self.zombie_environment = zombie_environment\n",
    "        self.target_values = target_values\n",
    "        self.errors = np.zeros(self.episodes)\n",
    "      \n",
    "        #initial state, action values\n",
    "        self.Q_S_A = np.zeros((self.number_of_states, self.number_of_actions))\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    def epsilon_policy(self, epsilon, Q_S_A, state, env):\n",
    "        '''\n",
    "        Applying epsilon greedy stratgy,\n",
    "        return the corresponding action given current epsilon\n",
    "        '''\n",
    "        prob = random.uniform(0, 1)\n",
    "        if prob < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_S_A[state])\n",
    "        return action\n",
    "   \n",
    "\n",
    "\n",
    "    def run_training(self):\n",
    "        for episode in tqdm(range(self.episodes)):\n",
    "            #restrat the environment after every episode\n",
    "            state, info = self.zombie_environment.reset()\n",
    "            epsilon =  1 / (1 + episode / 5000)#adjust epsilon to decay gradually                  \n",
    "            action = self.epsilon_policy(epsilon, self.Q_S_A, state, self.zombie_environment)\n",
    "            terminated = False\n",
    "            \n",
    "\n",
    "            while not terminated:\n",
    "                next_state, reward, terminated = self.zombie_environment.step(action)[:3]\n",
    "                next_action = self.epsilon_policy(epsilon, self.Q_S_A, next_state, self.zombie_environment)\n",
    "                #Apply sarsa state action update function\n",
    "                self.Q_S_A[state][action] += self.alpha * (reward + self.gamma * self.Q_S_A[next_state][next_action] -  self.Q_S_A[state][action])                    \n",
    "                #update state action for the next step of episode    \n",
    "                state, action = next_state, next_action\n",
    "                \n",
    "            # Every x episodes calculate the cumulative reward of the current policy\n",
    "            if episode % 100 == 0:\n",
    "                self.policy = np.argmax(self.Q_S_A, axis=1)\n",
    "                self.calc_policy_reward(episode_n = episode)\n",
    "                 \n",
    "            #Get the value table after each episode\n",
    "            self.value_function = np.max(self.Q_S_A, axis = 1)\n",
    "            \n",
    "            if self.target_values is not None:\n",
    "                self.store_error(episode)\n",
    "                \n",
    "            self.policy = np.argmax(self.Q_S_A, axis=1)\n",
    "        return self.Q_S_A, self.policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95191cef-fc16-4aef-a3a0-d9383fd652ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDQLearning(LearningAlgorithm):\n",
    "    def __init__(self, zombie_environment: ZombieEscapeEnv,\n",
    "                 episodes = 100000, alpha = 0.05, epsilon_start = 0.9, epsilon_end = 0.1, target_values = None):\n",
    "        super().__init__(zombie_environment)\n",
    "        self.zombie_environment = zombie_environment\n",
    "        self.episodes = episodes\n",
    "        self.gamma = zombie_environment.get_gamma()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon = epsilon_start\n",
    "\n",
    "        self.qsa = np.zeros((self.zombie_environment.observation_space.n, self.zombie_environment.action_space.n))\n",
    "        self.policy = np.zeros(self.zombie_environment.observation_space.n)\n",
    "\n",
    "        self.target_values = target_values\n",
    "        self.errors = np.zeros(self.episodes)\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_policy(epsilon, Q_S_A, state, env):\n",
    "        \"\"\"Choose random action or best action (according to QSA) based on random chance and epsilon\"\"\"\n",
    "        prob = random.uniform(0, 1)\n",
    "        if prob < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_S_A[state])\n",
    "        return action\n",
    "\n",
    "    def new_epsilon(self, episode_number):\n",
    "        \"\"\"Calculate epsilon for certain episode number based on decreasing (liner) epsilon formula\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end,\n",
    "                           self.epsilon_start - episode_number / self.episodes * (self.epsilon_start - self.epsilon_end))\n",
    "\n",
    "    def run_training(self):\n",
    "        for episode in tqdm(range(self.episodes)):\n",
    "            state = self.zombie_environment.reset()\n",
    "\n",
    "            terminated = False\n",
    "            while not terminated:\n",
    "                # Choose and execute action using epsilon greedy policy\n",
    "                action = self.epsilon_policy(self.epsilon, self.qsa, state, env=self.zombie_environment)\n",
    "                next_state, reward, terminated = self.zombie_environment.step(action)[:3]\n",
    "\n",
    "                # Update q values according to update rule\n",
    "                self.qsa[state, action] += self.alpha * (reward + self.gamma * np.max(self.qsa[next_state]) - self.qsa[state, action])\n",
    "\n",
    "                # Update current state to next state\n",
    "                state = next_state\n",
    "\n",
    "            # Every x episodes calculate the cumulative reward of the current policy\n",
    "            if episode % 100 == 0:\n",
    "                self.policy = np.argmax(self.qsa, axis=1)\n",
    "                self.calc_policy_reward(episode_n = episode)\n",
    "\n",
    "            # Calculate the value matrix and compare it target values to caclulate RMS\n",
    "            self.value_function = np.max(self.qsa, axis=1)\n",
    "            if self.target_values is not None:\n",
    "                self.store_error(episode)\n",
    "\n",
    "            # TODO: Better epsilon function, since it's currently under-performing a static epsilon\n",
    "            self.new_epsilon(episode)\n",
    "\n",
    "        # Calculate the final policy matrix and value matrix\n",
    "        self.policy = np.argmax(self.qsa, axis=1)\n",
    "        self.value_function = np.max(self.qsa, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880e2d4-0e28-45eb-8c9d-ff839665110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algs(alg_list: List[LearningAlgorithm]):\n",
    "    # Check if all algs have same amount of episodes\n",
    "    episode_amount = alg_list[0].episodes\n",
    "    for alg in alg_list[1::]:\n",
    "        assert alg.episodes == episode_amount, \"Episode amount should be equal for all algorithms to plot difference\"\n",
    "\n",
    "    # Train all algs\n",
    "    for alg in alg_list:\n",
    "        print(f\"Now training: {alg}\")\n",
    "        alg.get_training_results()\n",
    "\n",
    "    # Plot errors for all algs\n",
    "    index = range(len(alg_list[0].errors))\n",
    "    for alg in alg_list:\n",
    "        plt.plot(index, alg.errors, label=f\"{type(alg).__name__}\")\n",
    "\n",
    "    # Labels and visualisation\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Root Mean Squared Error')\n",
    "    plt.title(\"RMSE values in Q-Learning for different alphas\")\n",
    "    plt.legend(title=\"Algorithms\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
